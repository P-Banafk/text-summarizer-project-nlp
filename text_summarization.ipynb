{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8035c2c",
   "metadata": {},
   "source": [
    "## Importing Libs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ec0012-7b2c-4e84-89c1-b9b7bab57340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT0001\\Downloads\\text_summarizer-main\\text_summarizer-main\\texts\\Lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT0001\\Downloads\\text_summarizer-main\\text_summarizer-main\\texts\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIIT0001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns  \n",
    "import re \n",
    "import nltk\n",
    "import spacy\n",
    "import pytextrank\n",
    "import evaluate\n",
    "\n",
    "from transformers import pipeline\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8e2fc",
   "metadata": {},
   "source": [
    "## Loading Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008c3703-d104-42f1-9c97-b423e5b035ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('news_summary.csv', encoding='latin-1')\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load news_summary.csv: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d402e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (4514, 6)\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={'ctext': 'full_text', 'text': 'summary'}, inplace=True)\n",
    "print(f\"Original data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885abd18",
   "metadata": {},
   "source": [
    "## Cleaning dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65e7754-c5fc-4864-8233-2d32912c0438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after dropping NaNs: (4396, 8)\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Lowercase the text\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to both text and summary\n",
    "df['clean_text'] = df['summary'].apply(clean_text)\n",
    "\n",
    "df['clean_full_text'] = df['full_text'].apply(clean_text)\n",
    "\n",
    "df.dropna(subset=['full_text', 'summary'], inplace=True)\n",
    "print(f\"Data shape after dropping NaNs: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc687bba",
   "metadata": {},
   "source": [
    "## Extractive Summarization (TextRank Baseline):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576e7e1",
   "metadata": {},
   "source": [
    "### --- Create a Sample ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02ebe68-af10-4b5a-a213-aa43b8b98c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing a sample of 500 articles...\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 500\n",
    "df_sample = df.head(SAMPLE_SIZE).copy()\n",
    "print(f\"Processing a sample of {len(df_sample)} articles...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655adb9",
   "metadata": {},
   "source": [
    "### --- Loading model ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83e0941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model with pytextrank loaded.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "print(\"spaCy model with pytextrank loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a8c451",
   "metadata": {},
   "source": [
    "### --- Initialize Scorer and Score Lists ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e9c65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "249296e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TextRank summarization and ROUGE evaluation...\n",
      "  ...processed 100/500\n",
      "  ...processed 200/500\n",
      "  ...processed 300/500\n",
      "  ...processed 400/500\n",
      "  ...processed 500/500\n",
      "...Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting TextRank summarization and ROUGE evaluation...\")\n",
    "\n",
    "for i, row in enumerate(df_sample.itertuples()):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"  ...processed {i + 1}/{len(df_sample)}\")\n",
    "\n",
    "    article = row.full_text\n",
    "    reference_summary = row.summary\n",
    "\n",
    "    try:\n",
    "        # 1. Process the article with the nlp pipeline\n",
    "        doc = nlp(article)\n",
    "\n",
    "        # 2. Generate Summary (pytextrank)\n",
    "        # We'll ask for a summary of 3 sentences\n",
    "        # doc._.textrank.summary() returns a list of sentences\n",
    "        summary_sentences = [sent.text for sent in doc._.textrank.summary(limit_sentences=3)]\n",
    "        generated_summary = \" \".join(summary_sentences)\n",
    "\n",
    "        if not generated_summary:\n",
    "            continue\n",
    "            \n",
    "        # 3. Calculate ROUGE Score\n",
    "        scores = scorer.score(reference_summary, generated_summary)\n",
    "\n",
    "        # 4. Store F-measures\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any other processing errors\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"...Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c83aa",
   "metadata": {},
   "source": [
    "### --- Calculate and Print Average Scores ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e717968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- pytextrank Baseline ROUGE Scores (F-measure) ---\n",
      "Total articles successfully scored: 500 / 500\n",
      "Average ROUGE-1: 0.3929\n",
      "Average ROUGE-2: 0.1703\n",
      "Average ROUGE-L: 0.2482\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- pytextrank Baseline ROUGE Scores (F-measure) ---\")\n",
    "print(f\"Total articles successfully scored: {len(rouge1_scores)} / {SAMPLE_SIZE}\")\n",
    "\n",
    "if rouge1_scores:\n",
    "    print(f\"Average ROUGE-1: {np.mean(rouge1_scores):.4f}\")\n",
    "    print(f\"Average ROUGE-2: {np.mean(rouge2_scores):.4f}\")\n",
    "    print(f\"Average ROUGE-L: {np.mean(rougeL_scores):.4f}\")\n",
    "else:\n",
    "    print(\"No articles were successfully scored. Check your data or sample size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb721069",
   "metadata": {},
   "source": [
    "##  Abstractive Summarization with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "452ced00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1afa6",
   "metadata": {},
   "source": [
    "### Converting pandas DataFrame into a Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3452fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_dataset = Dataset.from_pandas(df)\n",
    "train_test_split = hg_dataset.train_test_split(train_size=1000, test_size=200, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80c27bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Dataset created:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['author', 'date', 'headlines', 'read_more', 'summary', 'full_text', 'clean_text', 'clean_full_text', '__index_level_0__'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['author', 'date', 'headlines', 'read_more', 'summary', 'full_text', 'clean_text', 'clean_full_text', '__index_level_0__'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hg_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "print(\"Hugging Face Dataset created:\")\n",
    "print(hg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9104689f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]c:\\Users\\KIIT0001\\Downloads\\text_summarizer-main\\text_summarizer-main\\texts\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1562.00 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 1198.87 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized dataset example:\n",
      "{'author': 'Abhishek Bansal', 'date': '18 Feb 2017,Saturday', 'headlines': 'Aussie players sledged me during warm-up game: Shreyas Iyer', 'read_more': 'http://indiatoday.intoday.in/story/shreyas-iyer-david-warner-matthew-wade-india-a-vs-australia/1/886144.html ', 'summary': 'India A batsman Shreyas Iyer has said he was sledged by Australian wicketkeeper Matthew Wade and David Warner on the second day of the ongoing practice match. \"They started to sledge and said \\'this guy does not have defence, he can play only attacking shots\\',\" said Iyer. \"I am used to this type of sledging,\" the batsman added.', 'full_text': 'Attacking young batsman from Mumbai, Shreyas Iyer, said he was sledged by wicketkeeper Matthew Wade and David Warner during the course of the day two of the ongoing three-day practice match between India A and Australia.Iyer slammed an unbeaten 85 off 93 balls against Australia to power India A to 176/4 in reply to the visitors\\' 469/7 declared.Iyer revealed that he was sledged by Wade and vice captain Warner when he was going great guns but it did not affect him as he was used to it following tours to Australia with the India A team in the past.\"They started to sledge and said \\'this guy does not have defense, he can play only attacking shots\\'. It was first Wade and then Warner too joined in. But I am used to this type of sledging, having toured Australia with India A teams in the past,\" Iyer said.Iyer also said he was confident that one day he will definitely get to play for India.\"I don\\'t think much on that, I try to play and score as many runs as possible. The side is packed at present, but I know I will get a chance definitely to play for the country,\" said the 22-year-old. Iyer has had a phenomenal run at the domestic level in his first two seasons before his form took a slight dip in the current season. Today, Iyer started in brilliant fashion by hitting Australia\\'s leading off-spinner Nathan Lyon for a first-ball six over long on.Later Iyer often stepped out to Lyon and left-arm spinner Stephen O\\'Keefe to hit them for a total of five sixes over long-on in an impressive display of attacking batting and was the biggest contributor to India A\\'s first innings score.Iyer said the lofted shots he played against Lyon and O\\'Keefe were not predetermined.\"I am happy they were not predetermined shots. It was important to take charge early on and spread out the field. It worked out well,\" Iyer said. ', 'clean_text': 'india a batsman shreyas iyer has said he was sledged by australian wicketkeeper matthew wade and david warner on the second day of the ongoing practice match they started to sledge and said this guy does not have defence he can play only attacking shots said iyer i am used to this type of sledging the batsman added', 'clean_full_text': 'attacking young batsman from mumbai shreyas iyer said he was sledged by wicketkeeper matthew wade and david warner during the course of the day two of the ongoing threeday practice match between india a and australiaiyer slammed an unbeaten 85 off 93 balls against australia to power india a to 1764 in reply to the visitors 4697 declarediyer revealed that he was sledged by wade and vice captain warner when he was going great guns but it did not affect him as he was used to it following tours to australia with the india a team in the pastthey started to sledge and said this guy does not have defense he can play only attacking shots it was first wade and then warner too joined in but i am used to this type of sledging having toured australia with india a teams in the past iyer saidiyer also said he was confident that one day he will definitely get to play for indiai dont think much on that i try to play and score as many runs as possible the side is packed at present but i know i will get a chance definitely to play for the country said the 22yearold iyer has had a phenomenal run at the domestic level in his first two seasons before his form took a slight dip in the current season today iyer started in brilliant fashion by hitting australias leading offspinner nathan lyon for a firstball six over long onlater iyer often stepped out to lyon and leftarm spinner stephen okeefe to hit them for a total of five sixes over longon in an impressive display of attacking batting and was the biggest contributor to india as first innings scoreiyer said the lofted shots he played against lyon and okeefe were not predeterminedi am happy they were not predetermined shots it was important to take charge early on and spread out the field it worked out well iyer said ', '__index_level_0__': 1103, 'input_ids': [21603, 10, 24655, 53, 1021, 3795, 7, 348, 45, 15810, 6, 4488, 60, 63, 9, 7, 27, 7975, 6, 243, 3, 88, 47, 3, 7, 13553, 26, 57, 29719, 10477, 9771, 26765, 11, 1955, 20055, 383, 8, 503, 13, 8, 239, 192, 13, 8, 4912, 386, 18, 1135, 1032, 1588, 344, 1547, 71, 11, 2051, 5, 196, 7975, 3, 7, 40, 265, 2726, 46, 73, 17349, 11989, 326, 3, 4271, 11607, 581, 2051, 12, 579, 1547, 71, 12, 3, 26782, 13572, 16, 8776, 12, 8, 2692, 31, 314, 3951, 21766, 10126, 5, 196, 7975, 5111, 24, 3, 88, 47, 3, 7, 13553, 26, 57, 26765, 11, 6444, 14268, 20055, 116, 3, 88, 47, 352, 248, 13731, 68, 34, 410, 59, 2603, 376, 38, 3, 88, 47, 261, 12, 34, 826, 8108, 12, 2051, 28, 8, 1547, 71, 372, 16, 8, 657, 535, 10273, 708, 12, 3, 7, 13553, 11, 243, 3, 31, 8048, 4024, 405, 59, 43, 4453, 6, 3, 88, 54, 577, 163, 20550, 6562, 31, 5, 94, 47, 166, 26765, 11, 258, 20055, 396, 3311, 16, 5, 299, 27, 183, 261, 12, 48, 686, 13, 3, 7, 109, 12720, 6, 578, 3, 28059, 2051, 28, 1547, 71, 2323, 16, 8, 657, 976, 27, 7975, 243, 5, 196, 7975, 92, 243, 3, 88, 47, 4881, 24, 80, 239, 3, 88, 56, 1728, 129, 12, 577, 21, 1547, 535, 196, 278, 31, 17, 317, 231, 30, 24, 6, 27, 653, 12, 577, 11, 2604, 38, 186, 3154, 38, 487, 5, 37, 596, 19, 7614, 44, 915, 6, 68, 27, 214, 27, 56, 129, 3, 9, 1253, 1728, 12, 577, 21, 8, 684, 976, 243, 8, 1630, 18, 1201, 18, 1490, 5, 27, 7975, 65, 141, 3, 9, 24230, 661, 44, 8, 4422, 593, 16, 112, 166, 192, 9385, 274, 112, 607, 808, 3, 9, 9927, 10823, 16, 8, 750, 774, 5, 1960, 6, 27, 7975, 708, 16, 6077, 2934, 57, 10849, 2051, 31, 7, 1374, 326, 18, 7, 3180, 687, 18050, 14360, 21, 3, 9, 166, 18, 3184, 1296, 147, 307, 30, 5, 3612, 449, 27, 7975, 557, 3, 14417, 91, 12, 14360, 11, 646, 18, 6768, 5404, 687, 7872, 411, 31, 439, 15, 15, 89, 15, 12, 1560, 135, 21, 3, 9, 792, 13, 874, 1296, 15, 7, 147, 307, 18, 106, 16, 46, 4423, 1831, 13, 20550, 3, 27759, 11, 47, 8, 2630, 13932, 12, 1547, 71, 31, 7, 166, 19714, 2604, 5, 196, 7975, 243, 8, 13871, 15, 26, 6562, 3, 88, 1944, 581, 14360, 11, 411, 31, 439, 15, 15, 89, 15, 130, 59, 554, 22755, 535, 196, 183, 1095, 79, 130, 59, 554, 22755, 6562, 5, 94, 47, 359, 12, 240, 1567, 778, 30, 11, 3060, 91, 8, 1057, 5, 94, 1279, 91, 168, 976, 27, 7975, 243, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1547, 71, 3795, 7, 348, 4488, 60, 63, 9, 7, 27, 7975, 65, 243, 3, 88, 47, 3, 7, 13553, 26, 57, 3746, 29719, 10477, 9771, 26765, 11, 1955, 20055, 30, 8, 511, 239, 13, 8, 4912, 1032, 1588, 5, 96, 10273, 708, 12, 3, 7, 13553, 11, 243, 3, 31, 8048, 4024, 405, 59, 43, 13613, 6, 3, 88, 54, 577, 163, 20550, 6562, 31, 976, 243, 27, 7975, 5, 96, 196, 183, 261, 12, 48, 686, 13, 3, 7, 109, 12720, 976, 8, 3795, 7, 348, 974, 5, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for 't5-small'\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# This prefix is a T5 requirement to tell it what task to do\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "# Set max lengths for your text. You can adjust these.\n",
    "# Articles longer than 512 tokens will be cut off (truncated).\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 150\n",
    "\n",
    "# Create the preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # Add the prefix to all articles\n",
    "    inputs = [prefix + doc for doc in examples['full_text']]\n",
    "    \n",
    "    # Tokenize the articles (inputs)\n",
    "    model_inputs = tokenizer(inputs, \n",
    "                             max_length=MAX_INPUT_LENGTH, \n",
    "                             truncation=True)\n",
    "\n",
    "    # Tokenize the summaries (labels)\n",
    "    # We use 'text_target' for the tokenizer in summarization\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['summary'], \n",
    "                           max_length=MAX_TARGET_LENGTH, \n",
    "                           truncation=True)\n",
    "\n",
    "    # Add the tokenized labels to our model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Now, apply this function to our entire dataset\n",
    "# The 'batched=True' part makes this run much faster\n",
    "tokenized_datasets = hg_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print(\"\\nTokenized dataset example:\")\n",
    "print(tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb3bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT0001\\AppData\\Local\\Temp\\ipykernel_6752\\1131988129.py:70: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 1:28:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.024103</td>\n",
       "      <td>23.906800</td>\n",
       "      <td>12.219300</td>\n",
       "      <td>20.551300</td>\n",
       "      <td>23.947900</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.976378</td>\n",
       "      <td>24.277800</td>\n",
       "      <td>12.636300</td>\n",
       "      <td>20.727400</td>\n",
       "      <td>24.295500</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.966240</td>\n",
       "      <td>24.331100</td>\n",
       "      <td>12.803200</td>\n",
       "      <td>20.872400</td>\n",
       "      <td>24.373800</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Saving model...\n",
      "Model saved to 't5-small-summarizer-final'\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- 1. Load ROUGE Metric for Evaluation ---\n",
    "# This is the same metric we'll use to compare against our baseline\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    This function is called by the Trainer to compute ROUGE scores.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode generated summaries (predictions)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in labels (which are padding tokens)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries (labels)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects newlines after each sentence\n",
    "    decoded_preds = [\"\\n\".join(pred.split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.split()) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # Extract the F-measures\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Get median generation length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.median(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "# --- 2. Load the Pre-trained T5 Model ---\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "# --- 3. Define Training Arguments ---\n",
    "# This object holds all the settings for the training run\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5-small-summarizer\",         # Where to save the model\n",
    "    eval_strategy=\"epoch\",             # Run evaluation at the end of each epoch\n",
    "    learning_rate=2e-5,                      # The learning rate\n",
    "    per_device_train_batch_size=8,           # Batch size for training\n",
    "    per_device_eval_batch_size=8,            # Batch size for evaluation\n",
    "    weight_decay=0.01,                       # Regularization\n",
    "    save_total_limit=3,                      # Only keep the 3 best checkpoints\n",
    "    num_train_epochs=3,                      # Number of times to go over the data\n",
    "    predict_with_generate=True,              # MUST be True for summarization\n",
    "    fp16=False,                              # Set to True if you have a modern GPU\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. Create the Data Collator ---\n",
    "# This pads your inputs and labels dynamically to the longest \n",
    "# sequence in a batch, which is more efficient.\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "\n",
    "# --- 5. Initialize the Trainer ---\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,  # Pass our ROUGE function\n",
    ")\n",
    "\n",
    "\n",
    "# --- 6. Start Training! ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# --- 7. Save the Final Model ---\n",
    "print(\"Training complete. Saving model...\")\n",
    "trainer.save_model(\"t5-small-summarizer-final\")\n",
    "print(\"Model saved to 't5-small-summarizer-final'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398f484",
   "metadata": {},
   "source": [
    "### loading fine tuned model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24dc96d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 200, but your input_length is only 60. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model loaded from t5-small-summarizer-final\n",
      "\n",
      "--- Example 1 ---\n",
      "**Original Article (first 100 chars):** On this day, 68 years ago, Mahatma Gandhi had breathed his last after being fatally shot while on hi...\n",
      "\n",
      "**Reference Summary:** Mahatma Gandhi was assassinated on January 30, 1948, during a prayer meeting at Delhi?s Birla House. He was shot thrice by Nathuram Godse. \"If I'm to die by the bullet of a mad man, I must do so smiling. God must be in my heart and on my lips,\" Gandhi had reportedly said two days before his assassination.\n",
      "\n",
      "**Generated Summary (T5):** 68 years ago, Mahatma Gandhi had breathed his last after being fatally shot while on his way to a prayer meeting at the Birla House. Here are some things to know about his assassination.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 2 ---\n",
      "**Original Article (first 100 chars):** A D-day veteran who jumped 15,000ft from a plane has become the oldest person in the world to skydiv...\n",
      "\n",
      "**Reference Summary:** Bryson William Verdun Hayes, at the age of 101 years and 38 days, became the oldest person in the world to skydive after jumping from 15,000 feet. Verdun took ten members of his family to skydive along with him. Interestingly, the 101-year-old had been presented with the Legion d'honneur for his heroic actions in World War II.\n",
      "\n",
      "**Generated Summary (T5):** Bryson William Verdun Hayes, from Croyde, Devon, jumped 15,000ft from a plane on Sunday, breaking the British record for the oldest skydiver in the world . he took to the skies with 10 members of his family at Sky\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Example 3 ---\n",
      "**Original Article (first 100 chars):** Hindu Mahasabha president Swami Omji Maharaj, one of the panelists in Rahul Kanwal's primetime show ...\n",
      "\n",
      "**Reference Summary:** Self-proclaimed religious leader Swami Omji Maharaj on a TV news channel has said that women are sexual beings and would attract sexual predators, the way jaggery attracts flies, if not fully dressed. The remarks made by Omji, a Bigg Boss 10 contestant, came amid the ongoing rage after Bengaluru mass molestation incident that happened on the New Year's Eve.\n",
      "\n",
      "**Generated Summary (T5):** Hindu Mahasabha president Swami Omji Maharaj made controversial remarks about women and held their dressing sensibilities accountable for sexual assaults they face in India. He said women, if not fully clothed, will attract sexual predators the way jaggery attracts \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Path where you saved your final model\n",
    "model_dir = \"t5-small-summarizer-final\" \n",
    "\n",
    "# Load the tokenizer separately to check length\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir) \n",
    "# Load the model using the pipeline\n",
    "summarizer = pipeline(\"summarization\", model=model_dir, tokenizer=model_dir)\n",
    "\n",
    "print(f\"Fine-tuned model loaded from {model_dir}\") \n",
    "\n",
    "# --- Get Samples ---\n",
    "num_samples_to_show = 3\n",
    "samples = hg_dataset['test'].select(range(num_samples_to_show))\n",
    "prefix = \"summarize: \"\n",
    "# Define max input length (same as used in training)\n",
    "MAX_INPUT_LENGTH = 512 \n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    # --- Truncate Input Manually (Fix for Warning 3) ---\n",
    "    # Encode, truncate, then decode back to string to ensure length limit\n",
    "    inputs_truncated = tokenizer.encode(\n",
    "        prefix + sample['full_text'], \n",
    "        max_length=MAX_INPUT_LENGTH, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\" # Return PyTorch tensor\n",
    "    )\n",
    "    truncated_article_text = tokenizer.decode(inputs_truncated[0], skip_special_tokens=True)\n",
    "    # Remove the prefix temporarily added during tokenization check if necessary\n",
    "    if truncated_article_text.startswith(prefix.strip()):\n",
    "         truncated_article_text = truncated_article_text[len(prefix.strip()):].strip()\n",
    "            \n",
    "    # Add prefix back for the actual summarizer input\n",
    "    final_input_text = prefix + truncated_article_text\n",
    "            \n",
    "    # --- Reference Summary ---\n",
    "    reference_summary = sample['summary']\n",
    "\n",
    "    # --- Generate Summary (Fix for Warning 2) ---\n",
    "    generated_summary = summarizer(final_input_text, \n",
    "                                   max_new_tokens=60,  # Explicitly control NEW tokens\n",
    "                                   min_length=30,      # Min words in summary still applies\n",
    "                                   num_beams=4,        \n",
    "                                   early_stopping=True)[0]['summary_text']\n",
    "\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"**Original Article (first 100 chars):** {sample['full_text'][:100]}...\")\n",
    "    print(f\"\\n**Reference Summary:** {reference_summary}\")\n",
    "    print(f\"\\n**Generated Summary (T5):** {generated_summary}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "texts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
